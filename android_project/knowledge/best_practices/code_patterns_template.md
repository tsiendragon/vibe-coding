# ä»£ç å®ç°æ¨¡å¼åº“

> **ç‰ˆæœ¬**: v1.0  
> **æœ€åæ›´æ–°**: YYYY-MM-DD  
> **ç»´æŠ¤Agent**: agent-code-reviewer

---

## ä»£ç æ¨¡å¼ç´¢å¼•

| æ¨¡å¼ID | æ¨¡å¼åç§° | åº”ç”¨åœºæ™¯ | å¤æ‚åº¦ | PyTorchç‰ˆæœ¬ | æ€§èƒ½å½±å“ |
|--------|----------|----------|--------|-------------|----------|
| CP-001 | Lightningæ¨¡å—å°è£… | æ ‡å‡†è®­ç»ƒå¾ªç¯ | ä½ | 1.8+ | æ— å½±å“ |
| CP-002 | åŠ¨æ€æ¨¡å‹æ„å»º | å¯é…ç½®æ¶æ„ | ä¸­ | 1.8+ | è½»å¾®å¼€é”€ |
| CP-003 | å†…å­˜é«˜æ•ˆè®­ç»ƒ | å¤§æ¨¡å‹è®­ç»ƒ | é«˜ | 2.0+ | æ˜¾è‘—ä¼˜åŒ– |
| CP-004 | å¤šGPUåˆ†å¸ƒå¼ | å¤§è§„æ¨¡è®­ç»ƒ | é«˜ | 1.8+ | æ€§èƒ½æå‡ |

---

## æ ¸å¿ƒå®ç°æ¨¡å¼

### CP-001: PyTorch Lightningæ ‡å‡†å°è£…æ¨¡å¼

#### æ¨¡å¼æ¦‚è¿°
**ç›®æ ‡**: æ ‡å‡†åŒ–è®­ç»ƒå¾ªç¯ï¼Œå‡å°‘æ ·æ¿ä»£ç 
**é€‚ç”¨åœºæ™¯**: 90%ä»¥ä¸Šçš„æ·±åº¦å­¦ä¹ é¡¹ç›®
**æ ¸å¿ƒä»·å€¼**: ä»£ç å¤ç”¨æ€§é«˜ï¼Œæ˜“äºç»´æŠ¤å’Œæµ‹è¯•

#### æ ‡å‡†å®ç°
```python
import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from typing import Any, Dict, Tuple

class StandardLightningModule(pl.LightningModule):
    """æ ‡å‡†Lightningæ¨¡å—æ¨¡æ¿
    
    Features:
    - è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
    - çµæ´»çš„ä¼˜åŒ–å™¨é…ç½®
    - å†…ç½®æŒ‡æ ‡è®°å½•
    - å¯é…ç½®çš„å­¦ä¹ ç‡è°ƒåº¦
    """
    
    def __init__(
        self,
        model: nn.Module,
        num_classes: int,
        learning_rate: float = 1e-3,
        weight_decay: float = 0.01,
        warmup_epochs: int = 5,
        max_epochs: int = 100,
        **kwargs
    ):
        super().__init__()
        
        # ä¿å­˜è¶…å‚æ•°åˆ°checkpoints
        self.save_hyperparameters(ignore=['model'])
        
        self.model = model
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # æŒ‡æ ‡è®¡ç®—
        self.train_acc = pl.metrics.Accuracy(task="multiclass", num_classes=num_classes)
        self.val_acc = pl.metrics.Accuracy(task="multiclass", num_classes=num_classes)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.model(x)
    
    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> Dict[str, Any]:
        """è®­ç»ƒæ­¥éª¤"""
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—å‡†ç¡®ç‡
        acc = self.train_acc(logits, y)
        
        # è®°å½•æŒ‡æ ‡
        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train/acc', acc, on_step=True, on_epoch=True, prog_bar=True)
        
        return {'loss': loss}
    
    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> Dict[str, Any]:
        """éªŒè¯æ­¥éª¤"""
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—å‡†ç¡®ç‡
        acc = self.val_acc(logits, y)
        
        # è®°å½•æŒ‡æ ‡
        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)
        
        return {'val_loss': loss, 'val_acc': acc}
    
    def configure_optimizers(self) -> Dict[str, Any]:
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨
        optimizer = AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=self.max_epochs,
            eta_min=self.learning_rate * 0.01
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'epoch',
                'frequency': 1,
                'monitor': 'val/loss'
            }
        }
    
    def configure_callbacks(self) -> list:
        """é…ç½®å›è°ƒå‡½æ•°"""
        callbacks = [
            pl.callbacks.ModelCheckpoint(
                monitor='val/acc',
                mode='max',
                save_top_k=3,
                filename='{epoch}-{val_acc:.3f}'
            ),
            pl.callbacks.EarlyStopping(
                monitor='val/loss',
                patience=10,
                mode='min'
            ),
            pl.callbacks.LearningRateMonitor(logging_interval='epoch')
        ]
        return callbacks
```

#### ä½¿ç”¨ç¤ºä¾‹
```python
# æ¨¡å‹å®šä¹‰
model = timm.create_model('resnet50', pretrained=True, num_classes=10)

# Lightningæ¨¡å—
lightning_module = StandardLightningModule(
    model=model,
    num_classes=10,
    learning_rate=1e-3,
    max_epochs=100
)

# è®­ç»ƒå™¨
trainer = pl.Trainer(
    max_epochs=100,
    accelerator='gpu',
    devices=1,
    precision=16,  # æ··åˆç²¾åº¦
    callbacks=lightning_module.configure_callbacks()
)

# å¼€å§‹è®­ç»ƒ
trainer.fit(lightning_module, train_dataloader, val_dataloader)
```

---

### CP-002: åŠ¨æ€æ¨¡å‹æ„å»ºæ¨¡å¼

#### æ¨¡å¼æ¦‚è¿°
**ç›®æ ‡**: é€šè¿‡é…ç½®æ–‡ä»¶åŠ¨æ€æ„å»ºä¸åŒçš„æ¨¡å‹æ¶æ„
**é€‚ç”¨åœºæ™¯**: éœ€è¦å®éªŒå¤šç§æ¶æ„çš„ç ”ç©¶é¡¹ç›®
**æ ¸å¿ƒä»·å€¼**: æ¶æ„æœç´¢ã€è¶…å‚æ•°ä¼˜åŒ–ã€æ¨¡å‹å¯¹æ¯”

#### é…ç½®é©±åŠ¨çš„æ¨¡å‹æ„å»º
```python
from dataclasses import dataclass
from typing import List, Optional, Union
import torch.nn as nn

@dataclass
class LayerConfig:
    """å±‚é…ç½®"""
    type: str  # 'conv2d', 'linear', 'attention'
    in_features: Optional[int] = None
    out_features: Optional[int] = None
    kernel_size: Optional[int] = None
    stride: Optional[int] = 1
    padding: Optional[int] = 0
    activation: Optional[str] = 'relu'
    dropout: Optional[float] = 0.0
    
@dataclass 
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    input_shape: List[int]
    num_classes: int
    layers: List[LayerConfig]

class DynamicModelBuilder:
    """åŠ¨æ€æ¨¡å‹æ„å»ºå™¨"""
    
    def __init__(self):
        self.layer_registry = {
            'conv2d': self._build_conv2d,
            'linear': self._build_linear,
            'attention': self._build_attention,
            'residual_block': self._build_residual_block
        }
        
        self.activation_registry = {
            'relu': nn.ReLU,
            'gelu': nn.GELU,
            'swish': nn.SiLU,
            'leaky_relu': nn.LeakyReLU
        }
    
    def build_model(self, config: ModelConfig) -> nn.Module:
        """æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹"""
        layers = []
        
        for layer_config in config.layers:
            layer = self._build_layer(layer_config)
            layers.append(layer)
            
            # æ·»åŠ æ¿€æ´»å‡½æ•°
            if layer_config.activation:
                activation = self.activation_registry[layer_config.activation]()
                layers.append(activation)
                
            # æ·»åŠ Dropout
            if layer_config.dropout > 0:
                layers.append(nn.Dropout(layer_config.dropout))
        
        # æ·»åŠ åˆ†ç±»å¤´
        layers.append(nn.AdaptiveAvgPool2d((1, 1)))
        layers.append(nn.Flatten())
        layers.append(nn.Linear(layers[-3].out_channels, config.num_classes))
        
        return nn.Sequential(*layers)
    
    def _build_layer(self, config: LayerConfig) -> nn.Module:
        """æ„å»ºå•ä¸ªå±‚"""
        if config.type not in self.layer_registry:
            raise ValueError(f"Unknown layer type: {config.type}")
        
        return self.layer_registry[config.type](config)
    
    def _build_conv2d(self, config: LayerConfig) -> nn.Module:
        """æ„å»ºå·ç§¯å±‚"""
        return nn.Conv2d(
            in_channels=config.in_features,
            out_channels=config.out_features,
            kernel_size=config.kernel_size,
            stride=config.stride,
            padding=config.padding
        )
    
    def _build_linear(self, config: LayerConfig) -> nn.Module:
        """æ„å»ºçº¿æ€§å±‚"""
        return nn.Linear(config.in_features, config.out_features)
        
    def _build_attention(self, config: LayerConfig) -> nn.Module:
        """æ„å»ºæ³¨æ„åŠ›å±‚"""
        return nn.MultiheadAttention(
            embed_dim=config.in_features,
            num_heads=8,
            dropout=config.dropout
        )
```

#### é…ç½®æ–‡ä»¶ç¤ºä¾‹
```yaml
# model_config.yaml
model:
  name: "custom_resnet"
  input_shape: [3, 224, 224]
  num_classes: 10
  layers:
    - type: "conv2d"
      in_features: 3
      out_features: 64
      kernel_size: 7
      stride: 2
      padding: 3
      activation: "relu"
      
    - type: "residual_block"
      in_features: 64
      out_features: 64
      num_blocks: 3
      
    - type: "conv2d"
      in_features: 64
      out_features: 128
      kernel_size: 3
      stride: 2
      padding: 1
      activation: "relu"
      dropout: 0.1
```

---

### CP-003: å†…å­˜é«˜æ•ˆè®­ç»ƒæ¨¡å¼

#### æ¨¡å¼æ¦‚è¿°
**ç›®æ ‡**: åœ¨æœ‰é™GPUå†…å­˜ä¸‹è®­ç»ƒå¤§æ¨¡å‹
**é€‚ç”¨åœºæ™¯**: å¤§æ¨¡å‹è®­ç»ƒã€GPUå†…å­˜ä¸è¶³åœºæ™¯
**æ ¸å¿ƒæŠ€æœ¯**: æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦

#### å†…å­˜ä¼˜åŒ–å®ç°
```python
import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint
from typing import Optional

class MemoryEfficientTrainer:
    """å†…å­˜é«˜æ•ˆè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        gradient_accumulation_steps: int = 4,
        use_gradient_checkpointing: bool = True,
        use_mixed_precision: bool = True,
        max_grad_norm: float = 1.0
    ):
        self.model = model
        self.optimizer = optimizer
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.max_grad_norm = max_grad_norm
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if use_mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
            
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if use_gradient_checkpointing:
            self._enable_gradient_checkpointing()
    
    def _enable_gradient_checkpointing(self):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜"""
        def checkpoint_wrapper(module):
            def forward(*args, **kwargs):
                return checkpoint(module.original_forward, *args, **kwargs)
            
            module.original_forward = module.forward
            module.forward = forward
            
        # å¯¹Transformerå±‚å¯ç”¨æ£€æŸ¥ç‚¹
        for name, module in self.model.named_modules():
            if 'layer' in name or 'block' in name:
                checkpoint_wrapper(module)
    
    def train_step(self, batch, criterion):
        """å†…å­˜é«˜æ•ˆçš„è®­ç»ƒæ­¥éª¤"""
        self.model.train()
        
        # å°†batchåˆ†æˆå°çš„sub-batches
        batch_size = len(batch[0])
        sub_batch_size = batch_size // self.gradient_accumulation_steps
        
        total_loss = 0
        self.optimizer.zero_grad()
        
        for i in range(self.gradient_accumulation_steps):
            # è·å–sub-batch
            start_idx = i * sub_batch_size
            end_idx = start_idx + sub_batch_size
            sub_batch = [x[start_idx:end_idx] for x in batch]
            
            # å‰å‘ä¼ æ’­
            if self.scaler:
                with torch.cuda.amp.autocast():
                    outputs = self.model(sub_batch[0])
                    loss = criterion(outputs, sub_batch[1])
                    loss = loss / self.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
            else:
                outputs = self.model(sub_batch[0])
                loss = criterion(outputs, sub_batch[1])
                loss = loss / self.gradient_accumulation_steps
                loss.backward()
            
            total_loss += loss.item()
            
            # æ¸…ç†ä¸­é—´å˜é‡ä»¥é‡Šæ”¾å†…å­˜
            del outputs, loss
            torch.cuda.empty_cache()
        
        # æ¢¯åº¦è£å‰ª
        if self.scaler:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
            self.optimizer.step()
        
        return total_loss
```

#### å†…å­˜ç›‘æ§å·¥å…·
```python
class MemoryMonitor:
    """GPUå†…å­˜ç›‘æ§å·¥å…·"""
    
    @staticmethod
    def get_memory_info():
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3     # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            return {
                'allocated': allocated,
                'cached': cached,
                'total': total,
                'free': total - cached
            }
        return None
    
    @staticmethod
    def print_memory_usage(stage: str = ""):
        """æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        info = MemoryMonitor.get_memory_info()
        if info:
            print(f"{stage} - GPU Memory: "
                  f"Allocated: {info['allocated']:.2f}GB, "
                  f"Cached: {info['cached']:.2f}GB, "
                  f"Free: {info['free']:.2f}GB")
```

---

### CP-004: å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼

#### æ¨¡å¼æ¦‚è¿°
**ç›®æ ‡**: é«˜æ•ˆåˆ©ç”¨å¤šGPUèµ„æºè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
**é€‚ç”¨åœºæ™¯**: å¤§æ•°æ®é›†ã€å¤§æ¨¡å‹è®­ç»ƒ
**æ ¸å¿ƒæŠ€æœ¯**: DDPã€FSDPã€æ¨¡å‹å¹¶è¡Œ

#### åˆ†å¸ƒå¼è®­ç»ƒå®ç°
```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import os

class DistributedTrainer:
    """åˆ†å¸ƒå¼è®­ç»ƒå™¨"""
    
    def __init__(self, backend: str = 'nccl'):
        self.backend = backend
        self.rank = int(os.environ.get('RANK', 0))
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        self._init_distributed()
    
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        if self.world_size > 1:
            dist.init_process_group(
                backend=self.backend,
                rank=self.rank,
                world_size=self.world_size
            )
            torch.cuda.set_device(self.local_rank)
    
    def setup_model(self, model: nn.Module) -> nn.Module:
        """è®¾ç½®åˆ†å¸ƒå¼æ¨¡å‹"""
        model = model.to(self.local_rank)
        
        if self.world_size > 1:
            model = DDP(
                model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False  # ä¼˜åŒ–æ€§èƒ½
            )
        
        return model
    
    def setup_dataloader(self, dataset, batch_size: int, shuffle: bool = True):
        """è®¾ç½®åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
        if self.world_size > 1:
            sampler = DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=shuffle
            )
            shuffle = False  # ä½¿ç”¨sampleræ—¶ä¸èƒ½åŒæ—¶shuffle
        else:
            sampler = None
        
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=4,
            pin_memory=True
        )
        
        return dataloader, sampler
    
    def is_main_process(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
        return self.rank == 0
    
    def save_checkpoint(self, model, optimizer, epoch, path):
        """ä¿å­˜æ£€æŸ¥ç‚¹(ä»…ä¸»è¿›ç¨‹)"""
        if self.is_main_process():
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }
            torch.save(checkpoint, path)
    
    def cleanup(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        if self.world_size > 1:
            dist.destroy_process_group()
```

#### å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# distributed_train.sh

export MASTER_ADDR="localhost"
export MASTER_PORT="12355"

python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    train_distributed.py \
    --batch_size 32 \
    --epochs 100
```

---

## æ€§èƒ½ä¼˜åŒ–æ¨¡å¼

### æ•°æ®åŠ è½½ä¼˜åŒ–
```python
class OptimizedDataLoader:
    """ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    
    @staticmethod
    def create_efficient_dataloader(
        dataset,
        batch_size: int,
        num_workers: int = 4,
        prefetch_factor: int = 2,
        persistent_workers: bool = True
    ):
        """åˆ›å»ºé«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨"""
        return torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,  # åŠ é€ŸGPUä¼ è¾“
            prefetch_factor=prefetch_factor,  # é¢„å–æ•°æ®
            persistent_workers=persistent_workers,  # ä¿æŒworkerè¿›ç¨‹
            drop_last=True  # ä¿æŒbatchå¤§å°ä¸€è‡´
        )
```

### æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–(PyTorch 2.0+)
```python
def optimize_model_for_inference(model: nn.Module) -> nn.Module:
    """ä¼˜åŒ–æ¨¡å‹ç”¨äºæ¨ç†"""
    # ç¼–è¯‘æ¨¡å‹(PyTorch 2.0+)
    if hasattr(torch, 'compile'):
        model = torch.compile(model, mode='max-autotune')
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # é¢„çƒ­
    dummy_input = torch.randn(1, 3, 224, 224).cuda()
    with torch.no_grad():
        for _ in range(10):
            _ = model(dummy_input)
    
    return model
```

---

## ä»£ç è´¨é‡æ£€æŸ¥æ¸…å•

### å¿…é¡»æ£€æŸ¥é¡¹ âœ…
- [ ] æ‰€æœ‰é­”æ•°éƒ½å®šä¹‰ä¸ºå¸¸é‡
- [ ] å‡½æ•°å’Œç±»éƒ½æœ‰ç±»å‹æ³¨è§£
- [ ] å…³é”®å‡½æ•°æœ‰docstringè¯´æ˜
- [ ] é”™è¯¯å¤„ç†è¦†ç›–ä¸»è¦å¼‚å¸¸æƒ…å†µ
- [ ] å†…å­˜æ³„æ¼æ£€æŸ¥(ç‰¹åˆ«æ˜¯å¾ªç¯å¼•ç”¨)
- [ ] GPUå†…å­˜é‡Šæ”¾(torch.cuda.empty_cache())

### æ€§èƒ½æ£€æŸ¥é¡¹ âš¡
- [ ] é¿å…ä¸å¿…è¦çš„tensor.cpu()è°ƒç”¨
- [ ] ä½¿ç”¨in-placeæ“ä½œå‡å°‘å†…å­˜åˆ†é…
- [ ] æ‰¹å¤„ç†è€Œéå¾ªç¯å¤„ç†
- [ ] åˆç†ä½¿ç”¨torch.no_grad()
- [ ] æ•°æ®é¢„å¤„ç†å¹¶è¡ŒåŒ–

### å¯ç»´æŠ¤æ€§æ£€æŸ¥é¡¹ ğŸ”§
- [ ] é…ç½®å’Œä»£ç åˆ†ç¦»
- [ ] æ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£å•ä¸€
- [ ] å¯å¤ç”¨çš„ç»„ä»¶æå–ä¸ºç±»
- [ ] æ—¥å¿—è®°å½•å…³é”®æ­¥éª¤
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–æ ¸å¿ƒé€»è¾‘

---

## åæ¨¡å¼è­¦å‘Š âš ï¸

### å¸¸è§é”™è¯¯æ¨¡å¼
```python
# âŒ é”™è¯¯: åœ¨å¾ªç¯ä¸­åˆ›å»ºtensor
for i in range(1000):
    x = torch.tensor([i])  # æ¯æ¬¡åˆ›å»ºæ–°tensor

# âœ… æ­£ç¡®: é¢„åˆ†é…tensor
x = torch.zeros(1000)
for i in range(1000):
    x[i] = i

# âŒ é”™è¯¯: ä¸å¿…è¦çš„GPU-CPUä¼ è¾“
for batch in dataloader:
    loss = criterion(model(batch[0]), batch[1])
    print(f"Loss: {loss.item()}")  # æ¯æ¬¡éƒ½ä¼ è¾“åˆ°CPU

# âœ… æ­£ç¡®: æ‰¹é‡è®°å½•
losses = []
for batch in dataloader:
    loss = criterion(model(batch[0]), batch[1])
    losses.append(loss.item())
print(f"Average loss: {sum(losses)/len(losses)}")
```

### å†…å­˜æ³„æ¼æ¨¡å¼
```python
# âŒ é”™è¯¯: ä¿æŒå¯¹è®¡ç®—å›¾çš„å¼•ç”¨
class BadModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.losses = []  # ä¿æŒå¯¹loss tensorçš„å¼•ç”¨
    
    def forward(self, x):
        output = self.linear(x)
        loss = self.criterion(output, target)
        self.losses.append(loss)  # å†…å­˜æ³„æ¼!
        return output

# âœ… æ­£ç¡®: åªä¿å­˜æ ‡é‡å€¼
class GoodModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.losses = []
    
    def forward(self, x):
        output = self.linear(x)
        loss = self.criterion(output, target)
        self.losses.append(loss.item())  # åªä¿å­˜æ•°å€¼
        return output
```